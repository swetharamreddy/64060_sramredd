---
title: "FML_Assignment_2"
author: "Swetha Ramreddy"
date: "2023-09-30"
output:
  pdf_document: default
  word_document: default
---

```{r}
library(class)
library(dplyr)
library(caret)
library(gmodels)
#loading data set
dataset_univ_bank<-read.csv("C:/Users/pc/Downloads/UniversalBank.csv")
head(dataset_univ_bank)

#removing unwanted columns i.e ID and Zip code
dataset_univ_bank_1<-dataset_univ_bank[,-1]
head(dataset_univ_bank_1)
dataset_univ_bank_1<-dataset_univ_bank_1[,-4]
head(dataset_univ_bank_1)
#converting personal loan as factor
dataset_univ_bank_1$Personal.Loan=as.factor(dataset_univ_bank_1$Personal.Loan)

#running is.na to check if there are any NA values
head(is.na(dataset_univ_bank_1))
any(is.na(dataset_univ_bank_1))

# Converting categorical variable into i.e education into dummy variables

#converting education into character
education<-as.character(dataset_univ_bank_1$Education)

dataset_univ_bank_2<-cbind(dataset_univ_bank_1[,-6],education)
head(dataset_univ_bank_2)

dummymodel<-dummyVars("~education",data = dataset_univ_bank_2)
educationdummy<-data.frame(predict(dummymodel,dataset_univ_bank_2))
head(educationdummy)

dataset_univ_bank_dummy<-cbind(dataset_univ_bank_2[,-12],educationdummy)
head(dataset_univ_bank_dummy)

#dividing data into training and testing set
set.seed(555)
train<-createDataPartition(dataset_univ_bank_dummy$Personal.Loan,p=0.60,list = FALSE)
train_set<-dataset_univ_bank_dummy[train,]
nrow(train_set)
validation_set<-dataset_univ_bank_dummy[-train,]
nrow(validation_set)
test_set<-data.frame(Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2,  Mortgage = 0, Securities.Account = 0, CD.Account = 0, Online = 1, 
      CreditCard = 1,education1 = 0, education2 = 1, education3 = 0)


summary(train_set)
summary(validation_set)
summary(test_set)

#normalizing

normvar<-c('Age',"Experience","Income","Family","CCAvg","Mortgage","Securities.Account","CD.Account","Online","CreditCard","education1","education2","education3")
normalization_values<-preProcess(train_set[,normvar],method = c('center','scale'))

train_set.norm<-predict(normalization_values,train_set)
summary(train_set.norm)

validation_set.norm<-predict(normalization_values,validation_set)
summary(validation_set.norm)

test_set.norm<-predict(normalization_values,test_set)
summary(test_set.norm)



#question 1: Classifying the given customer
set.seed(555)
new_grid<-expand.grid(k=c(1))
new_model<-train(Personal.Loan~.,data=train_set.norm,method="knn",tuneGrid=new_grid)

new_model

predict_test<-predict(new_model,test_set.norm)
predict_test

#question 2: identifying the best k
set.seed(555)
searchGrid <- expand.grid(k=seq(1:30))
model<-train(Personal.Loan~.,data=train_set.norm,method="knn",tuneGrid=searchGrid)
model

plot(model$results$k,model$results$Accuracy, type = 'o')


#finding the best k
best_k <- model$bestTune[[1]]
best_k

#question3:confusion matrix
library(gmodels)

train_label<-train_set.norm[,7]
validation_label<-validation_set.norm[,7]
test_label<-test_set.norm[,7]

predicted_validation_label<-knn(train_set.norm,validation_set.norm,cl=train_label,k=5)

CrossTable(x=validation_label,y=predicted_validation_label,prop.chisq = FALSE)

#question4:Classifying the given customer with best k
set.seed(555)
bestk_grid<-expand.grid(k=c(best_k))
bestk_model<-train(Personal.Loan~.,data=train_set.norm,method="knn",tuneGrid=bestk_grid)
bestk_model

bestk_test<-predict(bestk_model,test_set.norm)
bestk_test

#question5:confusion matrix for validation and training sets
#dividing dataset into traning, validation and testing set
set.seed(555)
train1<-createDataPartition(dataset_univ_bank_dummy$Personal.Loan,p=0.50,list = FALSE)
train_set_2<-dataset_univ_bank_dummy[train1,]
middle_set<-dataset_univ_bank_dummy[-train1,]
nrow(middle_set)
train2<-createDataPartition(middle_set$Personal.Loan,p=0.6,list = FALSE)
validation_set_2<-middle_set[train2,]
test_set_2<-middle_set[-train2,]

nrow(train_set_2)
nrow(validation_set_2)
nrow(test_set_2)

#normalizing trainset_2,validationset_2,testset_2

normvar<-c('Age',"Experience","Income","Family","CCAvg","Mortgage","Securities.Account","CD.Account","Online","CreditCard","education1","education2","education3")
normalization_values_2<-preProcess(train_set_2[,normvar],method = c('center','scale'))

train_set.norm_2<-predict(normalization_values_2,train_set_2)
summary(train_set.norm_2)

validation_set.norm_2<-predict(normalization_values_2,validation_set_2)
summary(validation_set.norm_2)

test_set.norm_2<-predict(normalization_values_2,test_set_2)
summary(test_set.norm_2)

#confusion matrix
library(gmodels)

train_label_2<-train_set.norm_2[,7]
validation_label_2<-validation_set.norm_2[,7]
test_label_2<-test_set.norm_2[,7]

predicted_validationlabel_2<-knn(train_set.norm_2,validation_set.norm_2,cl=train_label_2,k=best_k)

predicted_testlabel_2<-knn(train_set.norm_2,test_set.norm_2,cl=train_label_2,k=best_k)

confusion_matrix_1<-CrossTable(x=validation_label_2,y=predicted_validationlabel_2,prop.chisq = FALSE)
confusion_matrix_2<-CrossTable(x=test_label_2,y=predicted_testlabel_2,prop.chisq = FALSE)


validation_table<-table(validation_label_2,predicted_validationlabel_2)
confusionMatrix(validation_table)

test_table<-table(test_label_2,predicted_testlabel_2)
confusionMatrix(test_table)

# on comparing the confusion matrix of validation set and testing set it can be seen that accuracy and sensitivity of validation is slightly greater than test set.
```